# -*- coding: utf-8 -*-
"""Mini_Batch_GD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XLrGYQPAwxDLTYmd_VXEs3pn-W3O61zZ
"""

from sklearn.datasets import load_diabetes
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
import time
import random

X,y = load_diabetes(return_X_y=True)

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 2)

reg = LinearRegression()
reg.fit(X_train,y_train)

class MBGDRegressor:
  def __init__(self,learning_rate=0.01,epochs=100,batch_size = 100):
    self.coef_ = None
    self.intercept_ = None
    self.lr = learning_rate
    self.epochs = epochs
    self.batch_size = batch_size

  def fit(self,X_train,y_train):
    # init ur coefficients
    self.intercept_ = 0    # b0
    # all the coefficients will be equal to the number of cols
    # self.coef_ is an array
    self.coef_ = np.ones(X_train.shape[1])

    for i in range(int(X_train.shape[0]/self.batch_size)):

        idx = random.sample(range(X_train.shape[0]), self.batch_size)
        y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_
        intercept_derivative = -2*np.mean(y_train[idx] - y_hat)
        self.intercept_ = self.intercept_ - (self.lr * intercept_derivative)
        # getting coef_derivatives (array/matrix) :
        coef_derivatives= -2*np.dot((y_train[idx]-y_hat),X_train[idx])/self.batch_size
        # now we will update all the coefficients (b0,b1,b2....)
        self.coef_ = self.coef_ - (self.lr * (coef_derivatives))

    print(self.intercept_, self.coef_)

  def predict(self,X_test):
    return np.dot(X_test,self.coef_) + self.intercept_

mbgd = MBGDRegressor(batch_size= int(X_train.shape[0]/45),learning_rate=0.8,epochs=248)

mbgd.fit(X_train,y_train)

y_pred = mbgd.predict(X_test)

r2_score(y_test,y_pred)

# Learning Schedule : When we get near the solution we can change the Learning
# rates, we keep lowering the learning rate as we get near the solution